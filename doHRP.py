import os
import pickle as pk
from warnings import simplefilter
import numpy as np
import pandas as pd
import scipy.cluster.hierarchy as sch
from scipy.cluster.hierarchy import ClusterWarning

# Assumes 'classhrp.py' is in the same directory
from classhrp import GenerateSIMMAT, QueryApi

simplefilter("ignore", ClusterWarning)
np.errstate(divide='ignore')

# ==============================================================================
# SECTION 1: CORE HRP ALGORITHM HELPERS (Unchanged)
# ==============================================================================
def getIVP(cov, **kargs):
    ivp = 1. / np.diag(cov)
    ivp /= ivp.sum()
    return ivp

def getClusterVar(cov, cItems):
    cov_ = cov.loc[cItems, cItems]
    w_ = getIVP(cov_).reshape(-1, 1)
    cVar = np.dot(np.dot(w_.T, cov_), w_)[0, 0]
    return cVar

def getQuasiDiag(link):
    link = link.astype(int)
    sortIx = pd.Series([link[-1, 0], link[-1, 1]])
    numItems = link[-1, 3]
    while sortIx.max() >= numItems:
        sortIx.index = range(0, sortIx.shape[0] * 2, 2)
        df0 = sortIx[sortIx >= numItems]
        i, j = df0.index, df0.values - numItems
        sortIx[i] = link[j, 0]
        df0 = pd.Series(link[j, 1], index=i + 1)
        sortIx = pd.concat([sortIx, df0])
        sortIx = sortIx.sort_index()
        sortIx.index = range(sortIx.shape[0])
    return sortIx.tolist()

def getRecBipart(cov, sortIx):
    w = pd.Series(1.0, index=sortIx)
    cItems = [sortIx]
    while len(cItems) > 0:
        cItems = [i[j:k] for i in cItems for j, k in ((0, len(i) // 2), (len(i) // 2, len(i))) if len(i) > 1]
        for i in range(0, len(cItems), 2):
            cItems0, cItems1 = cItems[i], cItems[i + 1]
            cVar0, cVar1 = getClusterVar(cov, cItems0), getClusterVar(cov, cItems1)
            alpha = 1 - cVar0 / (cVar0 + cVar1)
            w[cItems0] *= alpha
            w[cItems1] *= 1 - alpha
    return w

def correlDist(corr):
    return ((1 - corr) / 2.) ** 0.5

def min_var(cov):
    icov = np.linalg.inv(np.matrix(cov))
    one = np.ones((icov.shape[0], 1))
    return (icov * one) / (one.T * icov * one)

# ==============================================================================
# SECTION 2: DATA FETCHING HELPER (Unchanged)
# ==============================================================================
def getFilingURLSin(tickers, api_key):
    total_response = pd.DataFrame()
    queryApi = QueryApi(api_key=api_key)
    ticker_str = ", ".join(map(str, tickers))
    for from_batch in range(0, 9800, 200):
        try:
            payload = {
                "query": {"query_string": {"query": f"ticker:({ticker_str}) AND formType:\"10-K\""}},
                "from": str(from_batch), "size": "200", "sort": [{"filedAt": {"order": "desc"}}]
            }
            response = queryApi.get_filings(payload)
            if not response["filings"]: break
            temp_data = pd.DataFrame.from_records(response['filings'])
            temp_data['fyear'] = pd.to_datetime(temp_data['periodOfReport'], errors='coerce').dt.year
            temp_data['filedAt'] = pd.to_datetime(temp_data['filedAt'].str[:10])
            temp_data = temp_data.groupby(["ticker", 'fyear']).head(1)
            total_response = pd.concat([total_response, temp_data])
        except Exception as e:
            print(f"An error occurred during filing fetch: {e}")
            break
    return total_response.reset_index(drop=True)

# ==============================================================================
# SECTION 3: MAIN ANALYSIS FUNCTION (MODIFIED)
# ==============================================================================
def run_hrp_analysis(input_csv_path, output_dir, run_id, years, api_key=None, 
                     preloaded_urls_path=None, preloaded_texts_path=None, 
                     sample_size=10, window_size=30):
    """
    Runs HRP analysis in online or offline mode.

    To run in offline mode, provide paths to preloaded data files. These files can
    be generated by running once in online mode and saving the necessary data.
    
    Args:
        api_key (str, optional): API key for sec-api.io. Required for online mode.
        preloaded_urls_path (str, optional): Path to pickled filing URL DataFrame.
        preloaded_texts_path (str, optional): Path to pickled filing texts dictionary.
        ... other args
    """
    is_offline = preloaded_urls_path and preloaded_texts_path
    if not is_offline and not api_key:
        raise ValueError("An API key must be provided for online mode.")
    print("Running using SEC-API." if is_offline else "Running using preloaded mode.")

    # --- Step 1: Load Price Data ---
    print("Step 1: Loading and preparing price data...")
    price_df = pd.read_csv(input_csv_path, index_col=0)
    price_df.index = pd.to_datetime(price_df.index)
    
    # --- Step 2: Get Filing Metadata (Online or Offline) ---
    print("Step 2: Identifying tickers with available 10-K filings...")
    preloaded_texts_data = None
    if is_offline:
        itit__ = pd.read_csv(preloaded_urls_path)
        itit__ = itit__.sort_values(['ticker','filedAt'], ascending=False)
        itit__['fyear'] = pd.to_datetime(itit__['periodOfReport'], errors='coerce').dt.year
        itit__['filedAt'] = pd.to_datetime(itit__['filedAt'].str[:10])
        itit__ = itit__.groupby(["ticker",'fyear']).head(1)
        with open(preloaded_texts_path, 'rb') as f: preloaded_texts_data = pk.load(f)
    else:
        itit__ = getFilingURLSin(price_df.columns, api_key)
    
    available_tickers = list(set(itit__.ticker.tolist()) & set(price_df.columns))

    # --- Step 3: Sample Tickers ---
    print(f"Step 3: Calculating returns and sampling {sample_size} tickers...")
    returns_df = np.log(price_df[available_tickers]).diff().dropna(how='all')
    if preloaded_texts_path is not None:
        sampled_tickers = returns_df.columns.tolist()
    else:
        sampled_tickers = returns_df.sample(n=sample_size, axis=1, random_state=99).columns.tolist()
    df = returns_df[sampled_tickers].dropna()
    print(f"Selected tickers: {sampled_tickers}")

    # --- Step 4: Generate Similarity Matrix (Online or Offline) ---
    print("Step 4: Generating text-based similarity matrices...")
    years_text = [years[0] - 1, years[1]]
    simmat_generator_args = {'TICKERS': sampled_tickers, 'YEARS': years_text}
    
    if is_offline:
        simmat_generator_args['preloaded_filing_urls'] = itit__[itit__['ticker'].isin(sampled_tickers)]
        simmat_generator_args['preloaded_filing_texts'] = {t: preloaded_texts_data.get(t, {}) for t in sampled_tickers}
    else:
        simmat_generator_args['api_key'] = api_key
        
    simmat_generator = GenerateSIMMAT(**simmat_generator_args)
    _, unclean = simmat_generator.create_simmat()
    
    if unclean.empty:
        print("Failed to generate similarity matrix. Aborting.")
        return

    # --- Nested Helper Function (Unchanged) ---
    def calculate_weights(df, tickers_list, keep_unclean):
        save_dict = {}
        cov, corr = df.cov(), df.corr()
        dist_hrp = correlDist(corr)
        link_hrp = sch.linkage(dist_hrp, 'single')
        sortIx_hrp = getQuasiDiag(link_hrp)
        hrp = getRecBipart(cov, corr.index[sortIx_hrp].tolist())
        save_dict['HRP'] = hrp.sort_values(ascending=False)
        tfidf = np.asmatrix(np.split(keep_unclean.values[0], len(tickers_list)))
        pairwise_similarity = np.asarray((tfidf * tfidf.T))
        dist_tbhrp = np.sqrt(((1 - pairwise_similarity) / 2.))
        np.fill_diagonal(dist_tbhrp, 0)
        link_tbhrp = sch.linkage(np.nan_to_num(dist_tbhrp, copy=False), 'single')
        sortIx_tbhrp = getQuasiDiag(link_tbhrp)
        tbhrp = getRecBipart(cov, corr.index[sortIx_tbhrp].tolist())
        save_dict['TBHRP'] = tbhrp.sort_values(ascending=False)
        save_dict['MV'] = pd.Series(np.asarray(min_var(cov).T)[0], cov.columns).sort_values(ascending=False)
        iv_weights = 1 / df.std()
        save_dict['IV'] = (iv_weights / iv_weights.sum()).sort_values(ascending=False)
        return save_dict

    # --- Steps 5, 6, 7 (Unchanged) ---
    print("Step 5: Aligning data and creating rolling windows...")
    df = df[(df.index >= unclean.index.min()) & (df.index <= unclean.index.max())]
    list_df = [df.iloc[i:i + window_size] for i in range(0, df.shape[0], window_size) if i+window_size <= df.shape[0]]
    print("Step 6: Running optimization over all windows...")
    total_save = []
    for i, frame in enumerate(list_df):
        max_date = frame.index.max()
        relevant_sim_matrix = unclean[(unclean.index <= max_date)]
        if relevant_sim_matrix.empty: continue
        total_save.append(calculate_weights(frame, sampled_tickers, relevant_sim_matrix.iloc[[-1]]))
    print("Step 7: Saving results to disk...")
    os.makedirs(output_dir, exist_ok=True)
    with open(os.path.join(output_dir, f"{run_id}_pandas_frames.pk"), 'wb') as f: pk.dump(list_df, f)
    with open(os.path.join(output_dir, f"{run_id}_weights.pk"), 'wb') as f: pk.dump(total_save, f)
    print("Analysis complete.")

# ==============================================================================
# SECTION 4: EXAMPLE USAGE (MODIFIED)
# ==============================================================================
if __name__ == '__main__':
    API_KEY = "YOUR_API_KEY_HERE" 
    INPUT_FILE = os.path.join("Data", "top500Total.csv")
    OUTPUT_DIR = "Results"
    RUN_ID = "123"
    YEARS = [2016, 2020]
    
    # --- CHOOSE YOUR MODE ---
    # To run OFFLINE, provide paths to your preloaded data files.
    # To run ONLINE, set these to None and provide your API_KEY above.
    PRELOADED_URLS_FILE = os.path.join("OfflineData", "filing_urls.pk")
    PRELOADED_TEXTS_FILE = os.path.join("OfflineData", "filing_texts.pk")
    
    # Set to None to run in online mode
    # PRELOADED_URLS_FILE = None
    # PRELOADED_TEXTS_FILE = None
    
    # --- Execution ---
    run_hrp_analysis(
        input_csv_path=INPUT_FILE, output_dir=OUTPUT_DIR, run_id=RUN_ID,
        years=YEARS, api_key=API_KEY,
        preloaded_urls_path=PRELOADED_URLS_FILE,
        preloaded_texts_path=PRELOADED_TEXTS_FILE
    )